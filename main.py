#%% Resume Project 1import pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsimport numpy as npimport yfinance as yffrom yahoofinancials import YahooFinancialsimport datetimefrom dateutil.relativedelta import relativedeltaimport tweepyimport csvimport timeimport refrom textblob import TextBlobfrom keys import api_key, api_secret_key, bearer_token, access_token, access_token_secret#%% Import Price Datanow = datetime.datetime.now()start = now + relativedelta(years=-2) + relativedelta(days=1)tesla = yf.download('TSLA',                   interval='1h',                   start=start,                    #end=,                   auto_adjust = True,                   progress=True)#%% Import Tweets# Twitter API credentialsconsumer_key = api_keyconsumer_secret = api_secret_keyaccess_key = access_tokenaccess_secret = access_token_secretdef get_all_tweets(screen_name):    # Twitter only allows access to a users most recent 3240 tweets with this method        # Authorize twitter, initialize tweepy    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)    auth.set_access_token(access_key, access_secret)    api = tweepy.API(auth)        # Initialize a list to hold all the tweepy Tweets    alltweets = []          # Make initial request for most recent tweets (200 is the maximum allowed count)    new_tweets = api.user_timeline(screen_name = screen_name,count=200)        # Save most recent tweets    alltweets.extend(new_tweets)        # Save the id of the oldest tweet less one    oldest = alltweets[-1].id - 1        # Keep grabbing tweets until there are no tweets left to grab    while len(new_tweets) > 0:                print("pause...")        time.sleep(5)        print("...pause ended")                print(f"getting tweets before {oldest}")                # All subsiquent requests use the max_id param to prevent duplicates        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)                # Save most recent tweets        alltweets.extend(new_tweets)                # Update the id of the oldest tweet less one        oldest = alltweets[-1].id - 1                print(f"...{len(alltweets)} tweets downloaded so far")        # Transform the tweepy tweets into a 2D array that will populate the csv     outtweets = [[tweet.id_str, tweet.created_at, tweet.text] for tweet in alltweets]        # Write the csv      #with open(f'new_{screen_name}_tweets.csv', 'w') as f:    #    writer = csv.writer(f)    #    writer.writerow(["id","created_at","text"])    #    writer.writerows(outtweets)    return alltweetstweets = get_all_tweets("elonmusk")#%% Clean tweetstweets_json = []for tweet in tweets:    tweets_json.append(tweet._json)    tweets_frame = pd.DataFrame(tweets_json)date = pd.to_datetime(tweets_frame.created_at)tweets_frame["Date"] = datetweets_frame.index = tweets_frame["Date"]tweets_frame_cleaned = tweets_frame[["id", "text", "truncated", "retweet_count", "favorite_count", "lang"]]user_list1 = []for entity in tweets_frame["entities"]:    user_list2 = []    for user in entity["user_mentions"]:        user_list2.append(user["name"])    user_list1.append(user_list2)    tweets_frame_cleaned["user_mentions"] = user_list1del access_key, access_secret, access_token, access_token_secret, api_key, api_secret_key, bearer_token, consumer_key, consumer_secret, date, entity, now, start, tweets, tweet, tweets_json, user, user_list1, user_list2#%% EDA and Prep - Prices# Get and plot close pricesprices = tesla.Closeprices.plot()# MA and break pointsmoving_avg = prices.rolling(window=100, center=False).mean()moving_avg.plot()plt.show()log_returns = np.log(prices).diff()log_returns.plot()def assign_class(ret, thereshold=0.01):    if ret > thereshold:        return 1    elif ret < -thereshold:        return -1    else:        return 0        classes = log_returns.apply(assign_class)#%% EDA - Tweetsdef clean_tweet(tweet):    '''    Utility function to clean tweet text by removing links, special characters    using simple regex statements.    '''    return ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)", " ", tweet).split())def get_tweet_sentiment(tweet):    '''    Utility function to classify sentiment of passed tweet    using textblob's sentiment method    '''    # create TextBlob object of passed tweet text    analysis = TextBlob(clean_tweet(tweet))    # set sentiment    if analysis.sentiment.polarity > 0:        return 'positive'    elif analysis.sentiment.polarity == 0:        return 'neutral'    else:        return 'negative'        if __name__ == "__main__":   print("im a bg")